{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7146d10f",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23816c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assumes that the samla-dataset is stored in the data-folder within this project\n",
    "#rootdir = '.\\data\\samla'\n",
    "rootdir = f'{os.getcwd()}/data/samla'\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())  # Debugging statement\n",
    "\n",
    "samla = {}\n",
    "\n",
    "# Traversing every subfolder of the given folder in the 'rootdir' variable and finds\n",
    "# every file. Stores the files in the samla-dictionary defined above.\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for directories in dirs:\n",
    "        path = os.path.join(rootdir, directories)\n",
    "        for subdir, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if(file.startswith('.')):\n",
    "                    continue\n",
    "                #print(path)\n",
    "                #break\n",
    "                filePath = os.path.join(subdir, file)\n",
    "                #print(\"Trying to open file:\", filePath)  # Debugging statement\n",
    "                #print()\n",
    "                if os.path.exists(filePath):\n",
    "                    with open(filePath, 'rb') as f: #, encoding='iso-8859-1'\n",
    "                        if(samla.get(filePath) is not None):\n",
    "                            samla[filePath] = samla[filePath] + f.read()\n",
    "                        else:\n",
    "                            samla[filePath] = f.read()\n",
    "                else:\n",
    "                    print(f\"File not found: {filePath}\")  # Debugging statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf52e3",
   "metadata": {},
   "source": [
    "## Decoding unicode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(samla.items(), columns=['File', 'Text'])\n",
    "df['File'] = df['File']\n",
    "# Decoding it to become unicode characters\n",
    "txt = 0\n",
    "xml = 0\n",
    "for i in range(len(df)):\n",
    "    if df['File'][i].endswith(\".txt\"):\n",
    "        txt += 1\n",
    "        df['Text'][i] = df['Text'][i].decode(\"iso-8859-1\", \"strict\")\n",
    "    else:\n",
    "        df['Text'][i] = df['Text'][i].decode(\"utf-8\", \"replace\")\n",
    "        xml +=1\n",
    "\n",
    "txt, xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc839bb",
   "metadata": {},
   "source": [
    "## Cleaning text-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e32d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = df.iloc[0, :][\"Text\"]\n",
    "\n",
    "pattern = r\"<DATO>(.*?)<\\/DATO>.*?<STAD>(.*?)<\\/STAD>.*`?<TEKST>(.*?)<\\/TEKST>\"\n",
    "matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "#for match in matches:\n",
    "#    dato = match[0]\n",
    "#    tekst = match[1]\n",
    "#    print(f\"Dato: {dato}, Tekst: {tekst}\")\n",
    "matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffcf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pd.DataFrame(columns=[\"Concatenated\", \"Text\", \"Place\", \"Date\", \"Type\"])\n",
    "\n",
    "pattern = r\"<DATO>(.*?)<\\/DATO>.*?<STAD>(.*?)<\\/STAD>.*`?<TEKST>(.*?)<\\/TEKST>\"\n",
    "skip = 0\n",
    "skiplist = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.iloc[i, :][\"Text\"]\n",
    "    pattern = r\"<DATO>(.*?)<\\/DATO>.*?<STAD>(.*?)<\\/STAD>.*`?<TEKST>(.*?)<\\/TEKST>\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "        \n",
    "    # This is the case for 'minneoppgaver' or the two meta files about minneoppgaver (which we skip)\n",
    "    if len(matches) == 0:\n",
    "        \n",
    "        # FOR MINNEOPPGAVER\n",
    "        pattern = r\"<title>(.*?)<\\/title>.*?<date>(.*?)<\\/date>.*?<body>(.*?)<\\/body>\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        \n",
    "        if len(matches) == 0:\n",
    "            skip += 1\n",
    "            skiplist.append(text)\n",
    "            continue\n",
    "        \n",
    "        # Converting to list from tuple to assign items\n",
    "        matches = list(matches[0])\n",
    "        \n",
    "        matched_text = matches[2]\n",
    "        \n",
    "        # Removing tags and whitespace from main-string\n",
    "        clean_string = re.sub(r'<\\w+(\\s+\\w+=\".*\")*/?>', '', matched_text)\n",
    "        clean_string = re.sub(r'</\\w+>', '', clean_string)\n",
    "        clean_string = re.sub(r'\\s+', ' ', clean_string)\n",
    "        \n",
    "        matches[2] = clean_string\n",
    "\n",
    "        # FOR MINNEOPPGAVER, IKKE FJERN DENNE MED MINDRE DU KLARER Å FÅ MINNEOPPGAVER PÅ FORMAT SOM IKKE ER FUCKED\n",
    "        transformed_df.loc[i] = [matches[1] + \" \" + matches[0] + \" \" + matches[2], matches[2], matches[0], matches[1], \"Minneoppgave\"]\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    # Converting to list from tuple to assign items\n",
    "    matches = list(matches[0])\n",
    "    \n",
    "    matched_text = matches[2]\n",
    "    \n",
    "    # Removing tags and whitespace from main-string\n",
    "    clean_string = re.sub(r'<\\w+(\\s+\\w+=\".*\")*/?>', '', matched_text)\n",
    "    clean_string = re.sub(r'</\\w+>', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\\s+', ' ', clean_string)\n",
    "        \n",
    "    matches[2] = clean_string\n",
    "\n",
    "    \n",
    "    transformed_df.loc[i] = [matches[0] + \" \" + matches[1] + \" \" + matches[2], matches[2], matches[1], matches[0], \"Eventyr/Sagn\"]\n",
    "\n",
    "print(skip)    \n",
    "transformed_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810bdef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KODE FOR Å SE HVA SOM GIKK GALT UNDER INNLASTNING, IKKE FJERN\n",
    "\n",
    "#text = skiplist[3]\n",
    "\n",
    "#pattern = r\"<title>(.*?)<\\/title>.*?<date>(.*?)<\\/date>.*?<body>(.*?)<\\/body>\"\n",
    "#matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "#print(matches)\n",
    "sample_string = transformed_df.iloc[500, :]\n",
    "\n",
    "#clean_string = re.sub(r'<\\w+(\\s+\\w+=\".*\")*/?>', '', sample_string)\n",
    "#clean_string = re.sub(r'</\\w+>', '', clean_string)\n",
    "#clean_string = re.sub(r'\\s+', ' ', clean_string)\n",
    "\n",
    "#clean_string\n",
    "print(sample_string[\"Type\"])\n",
    "sample_string[\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4a830",
   "metadata": {},
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "sentence_model = SentenceTransformer(\"NbAiLab/nb-sbert-base\")\n",
    "kw_model = KeyBERT(model=sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekster = transformed_df[\"Text\"].values\n",
    "\n",
    "topic_model = BERTopic(embedding_model='NbAiLab/nb-sbert-base').fit(tekster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78244754",
   "metadata": {},
   "source": [
    "Visualisation with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(tekster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54572182",
   "metadata": {},
   "outputs": [],
   "source": [
    "topictext = topic_model.get_document_info(tekster).iloc[1267, :][\"Document\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaaf85",
   "metadata": {},
   "source": [
    "Visualization with eventyr/sagn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea64bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekster = transformed_df.loc[transformed_df[\"Type\"] == \"Eventyr/Sagn\"][\"Text\"].values\n",
    "\n",
    "tp_es = BERTopic(embedding_model='NbAiLab/nb-sbert-base').fit(tekster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_es.visualize_documents(tekster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vectorizer_model = CountVectorizer(stop_words=stopwords.words(\"norwegian\"), ngram_range=(1, 5))\n",
    "#tp_es.update_topics(tekster, vectorizer_model=vectorizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1770e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_es.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29329e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_es.visualize_documents(tekster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903a12c",
   "metadata": {},
   "source": [
    "Visualization with minneoppgaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e78217",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_tekster = transformed_df.loc[transformed_df[\"Type\"] == \"Minneoppgave\"][\"Text\"].values\n",
    "\n",
    "tp_mo = BERTopic(embedding_model='NbAiLab/nb-sbert-base').fit(mo_tekster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15506a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_mo.visualize_documents(mo_tekster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93046e9",
   "metadata": {},
   "source": [
    "Trying to update topics to get a more exciting representation than the regular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdb635",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_mo.update_topics(mo_tekster, n_gram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef382ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_mo.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb737f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=stopwords.words(\"norwegian\"), ngram_range=(1, 5))\n",
    "tp_mo.update_topics(mo_tekster, vectorizer_model=vectorizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b692c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_mo.get_topics(tekster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_mo.visualize_documents(mo_tekster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
